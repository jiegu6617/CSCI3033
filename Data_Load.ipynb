{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Data_Load.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"6a0IUfJGPryK"},"source":["import numpy as np\n","np.random.seed(2016)\n","\n","import os\n","import glob\n","import cv2\n","import math\n","import pickle\n","import datetime\n","import pandas as pd\n","import statistics\n","import random\n","import time\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import KFold\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout, Activation, Flatten\n","from keras.layers.convolutional import Convolution2D, MaxPooling2D\n","from keras.optimizers import SGD, Adam\n","from keras.utils import np_utils\n","from keras.models import model_from_json\n","from sklearn.metrics import log_loss\n","import scipy.misc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MqboTp2FPugi"},"source":["import pickle\n","import datetime\n","import pandas as pd\n","import keras\n","\n","from keras.layers import BatchNormalization\n","from keras.models import Sequential\n","from keras.layers.core import Dense, Dropout, Flatten\n","from keras.layers.convolutional import Convolution2D, MaxPooling2D, \\\n","                                       ZeroPadding2D\n","\n","from keras.optimizers import SGD\n","from keras.utils import np_utils\n","from keras.models import model_from_json\n","from numpy.random import permutation\n","import time"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQUdJeMjPeJx"},"source":["# First transformation"]},{"cell_type":"code","metadata":{"id":"6yi2_KWEOJvr"},"source":["#Define dimension\n","img_rows, img_cols = 64, 64\n","batch_size = 32\n","nb_epoch = 1\n","random_state = 51\n","\n","\n","#data load functions\n","use_cache = 1\n","# color type: 1 - grey, 3 - rgb\n","color_type_global = 1\n","# color_type = 1 - gray\n","# color_type = 3 - RGB\n","def get_im_cv2(path, img_rows, img_cols, color_type=1):\n","    # Load as grayscale\n","    if color_type == 1:\n","        img = cv2.imread(path, 0)\n","    elif color_type == 3:\n","        img = cv2.imread(path)\n","    # Reduce size\n","    resized = cv2.resize(img, (img_cols, img_rows))\n","    return resized\n","\n","\n","def get_im_cv2_mod(path, img_rows, img_cols, color_type=1):\n","    # Load as grayscale\n","    if color_type == 1:\n","        img = cv2.imread(path, 0)\n","    else:\n","        img = cv2.imread(path)\n","    # Reduce size\n","    rotate = random.uniform(-10, 10)\n","    M = cv2.getRotationMatrix2D((img.shape[1]/2, img.shape[0]/2), rotate, 1)\n","    img = cv2.warpAffine(img, M, (img.shape[1], img.shape[0]))\n","    resized = cv2.resize(img, (img_cols, img_rows), cv2.INTER_LINEAR)\n","    return resized\n","    \n","\n","def get_driver_data():\n","    dr = dict()\n","    # path = os.path.join('..', 'input', 'driver_imgs_list.csv')\n","    path='/content/drive/MyDrive/deep_learning_project/input/driver_imgs_list.csv'\n","    print('Read drivers data')\n","    f = open(path, 'r')\n","    line = f.readline()\n","    while (1):\n","        line = f.readline()\n","        if line == '':\n","            break\n","        arr = line.strip().split(',')\n","        dr[arr[2]] = arr[0]\n","    f.close()\n","    return dr\n","\n","\n","def load_train(img_rows, img_cols, color_type=1):\n","    X_train = []\n","    y_train = []\n","    driver_id = []\n","    start_time = time.time()\n","    driver_data = get_driver_data()\n","\n","\n","\n","    print('Read train images')\n","    for j in range(10):\n","        print('Load folder c{}'.format(j))\n","        path='/content/drive/MyDrive/deep_learning_project/input/train/c'+str(j)+'/*.jpg'\n","        files = glob.glob(path)\n","        i=0\n","        for fl in files:\n","          if '(' not in fl:\n","            flbase = os.path.basename(fl)\n","            img = get_im_cv2_mod(fl, img_rows, img_cols, color_type=1)\n","            print('Loading image{}'.format(i))\n","            i = i+1\n","            X_train.append(img)\n","            y_train.append(j)\n","            driver_id.append(driver_data[flbase])\n","\n","    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n","    unique_drivers = sorted(list(set(driver_id)))\n","    print('Unique drivers: {}'.format(len(unique_drivers)))\n","    print(unique_drivers)\n","    return X_train, y_train, driver_id, unique_drivers\n","\n","\n","def load_test(img_rows, img_cols, color_type=1):\n","    print('Read test images')\n","    start_time = time.time()\n","    # path = os.path.join('..', 'input', 'test', '*.jpg')\n","    path='/content/drive/MyDrive/deep_learning_project/input/test/*.jpg'\n","    files = glob.glob(path)\n","    X_test = []\n","    X_test_id = []\n","    total = 0\n","    thr = math.floor(len(files)/10)\n","    i = 0\n","    for fl in files:\n","      if '(' not in fl:\n","        flbase = os.path.basename(fl)\n","        img = get_im_cv2_mod(fl, img_rows, img_cols, color_type)\n","        X_test.append(img)\n","        X_test_id.append(flbase)\n","        print('Loading image{}'.format(i))\n","        i = i+1\n","        total += 1\n","        if total%thr == 0:\n","            print('Read {} images from {}'.format(total, len(files)))\n","    \n","    print('Read test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n","    return X_test, X_test_id\n","\n","\n","def cache_data(data, path):\n","    if os.path.isdir(os.path.dirname(path)):\n","        file = open(path, 'wb')\n","        pickle.dump(data, file)\n","        file.close()\n","    else:\n","        print('Directory doesnt exists')\n","\n","\n","def restore_data(path):\n","    data = dict()\n","    if os.path.isfile(path):\n","        file = open(path, 'rb')\n","        data = pickle.load(file)\n","    return data\n","\n","\n","def save_model(model):\n","    json_string = model.to_json()\n","    if not os.path.isdir('cache'):\n","        os.mkdir('cache')\n","    open(os.path.join('cache', 'architecture.json'), 'w').write(json_string)\n","    model.save_weights(os.path.join('cache', 'model_weights.h5'), overwrite=True)\n","\n","\n","def read_model():\n","    model = model_from_json(open(os.path.join('cache', 'architecture.json')).read())\n","    model.load_weights(os.path.join('cache', 'model_weights.h5'))\n","    return model\n","\n","\n","def split_validation_set(train, target, test_size):\n","    random_state = 51\n","    X_train, X_test, y_train, y_test = train_test_split(train, target, test_size=test_size, random_state=random_state)\n","    return X_train, X_test, y_train, y_test\n","\n","\n","def create_submission(predictions, test_id, info):\n","    result1 = pd.DataFrame(predictions, columns=['c0', 'c1', 'c2', 'c3', 'c4', 'c5', 'c6', 'c7', 'c8', 'c9'])\n","    result1.loc[:, 'img'] = pd.Series(test_id, index=result1.index)\n","    now = datetime.datetime.now()\n","    if not os.path.isdir('subm'):\n","        os.mkdir('subm')\n","    suffix = info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\"))\n","    sub_file = os.path.join('subm', 'submission_' + suffix + '.csv')\n","    result1.to_csv(sub_file, index=False)\n","\n","\n","def read_and_normalize_train_data(img_rows, img_cols, color_type=1):\n","    cache_path = os.path.join('cache', 'train_r_' + str(img_rows) + '_c_' + str(img_cols) + '_t_' + str(color_type) + '.dat')\n","    if not os.path.isfile(cache_path) or use_cache == 0:\n","        train_data, train_target, driver_id, unique_drivers = load_train(img_rows, img_cols, color_type)\n","        cache_data((train_data, train_target, driver_id, unique_drivers), cache_path)\n","    else:\n","        print('Restore train from cache!')\n","        (train_data, train_target, driver_id, unique_drivers) = restore_data(cache_path)\n","\n","    train_data = np.array(train_data, dtype=np.uint8)\n","    train_target = np.array(train_target, dtype=np.uint8)\n","    train_data = train_data.reshape(train_data.shape[0], color_type, img_rows, img_cols)\n","    train_target = np_utils.to_categorical(train_target, 10)\n","    train_data = train_data.astype('float32')\n","    train_data /= 255\n","    print('Train shape:', train_data.shape)\n","    print(train_data.shape[0], 'train samples')\n","    return train_data, train_target, driver_id, unique_drivers\n","\n","\n","def read_and_normalize_test_data(img_rows, img_cols, color_type=1):\n","    cache_path = os.path.join('cache', 'test_r_' + str(img_rows) + '_c_' + str(img_cols) + '_t_' + str(color_type) + '.dat')\n","    if not os.path.isfile(cache_path) or use_cache == 0:\n","        test_data, test_id = load_test(img_rows, img_cols, color_type)\n","        cache_data((test_data, test_id), cache_path)\n","    else:\n","        print('Restore test from cache!')\n","        (test_data, test_id) = restore_data(cache_path)\n","\n","    test_data = np.array(test_data, dtype=np.uint8)\n","    test_data = test_data.reshape(test_data.shape[0], color_type, img_rows, img_cols)\n","    test_data = test_data.astype('float32')\n","    test_data /= 255\n","    print('Test shape:', test_data.shape)\n","    print(test_data.shape[0], 'test samples')\n","    return test_data, test_id\n","\n","\n","def dict_to_list(d):\n","    ret = []\n","    for i in d.items():\n","        ret.append(i[1])\n","    return ret\n","\n","\n","def merge_several_folds_mean(data, nfolds):\n","    a = np.array(data[0])\n","    for i in range(1, nfolds):\n","        a += np.array(data[i])\n","    a /= nfolds\n","    return a.tolist()\n","\n","\n","def merge_several_folds_geom(data, nfolds):\n","    a = np.array(data[0])\n","    for i in range(1, nfolds):\n","        a *= np.array(data[i])\n","    a = np.power(a, 1/nfolds)\n","    return a.tolist()\n","\n","\n","def copy_selected_drivers(train_data, train_target, driver_id, driver_list):\n","    data = []\n","    target = []\n","    index = []\n","    for i in range(len(driver_id)):\n","        if driver_id[i] in driver_list:\n","            data.append(train_data[i])\n","            target.append(train_target[i])\n","            index.append(i)\n","    data = np.array(data, dtype=np.float32)\n","    target = np.array(target, dtype=np.float32)\n","    index = np.array(index, dtype=np.uint32)\n","    return data, target, index\n","\n","\n","#Load data\n","train_data, train_target, driver_id, unique_drivers = read_and_normalize_train_data(img_rows, img_cols, color_type_global)\n","test_data, test_id = read_and_normalize_test_data(img_rows, img_cols, color_type_global)\n","\n","\n","\n","#Save data\n","np.save('train_data.npy', train_data)\n","np.save('train_target.npy', train_target)\n","np.save('driver_id.npy',driver_id)\n","np.save('unique_drivers.npy',unique_drivers)\n","\n","np.save('test_data.npy', test_data)\n","np.save('test_id.npy', test_id)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_MHQA_6cQSXN"},"source":["# Another transformation for loading data"]},{"cell_type":"code","metadata":{"id":"meUfcWRrOJon"},"source":["train_image = []\n","image_label = []\n","\n","#Load Data\n","for i in range(10):\n","    print('now we are in the folder C',i)\n","    imgs = os.listdir(\"/content/drive/MyDrive/deep_learning_project/input/train/c\"+str(i))\n","    for j in range(len(imgs)):\n","        img_name = \"/content/drive/MyDrive/deep_learning_project/input/train/c\"+str(i)+\"/\"+imgs[j]\n","        img = cv2.imread(img_name)\n","        img = img[50:,120:-50]\n","        img = cv2.resize(img,(224,224))\n","        label = i\n","        driver = driver_details[driver_details['img'] == imgs[j]]['subject'].values[0]\n","        train_image.append([img,label,driver])\n","        image_label.append(i)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVRyFI3ROJfp"},"source":["import random\n","random.shuffle(train_image)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n_U37AN2QwcG"},"source":["driv_selected = ['p050', 'p015', 'p022', 'p056']\n","\n","\n","# Splitting the train and test\n","X_train= []\n","y_train = []\n","X_test = []\n","y_test = []\n","D_train = []\n","D_test = []\n","\n","for features,labels,drivers in train_image:\n","    if drivers in driv_selected:\n","        X_test.append(features)\n","        y_test.append(labels)\n","        D_test.append(drivers)\n","    \n","    else:\n","        X_train.append(features)\n","        y_train.append(labels)\n","        D_train.append(drivers)\n","    \n","print (len(X_train),len(X_test))\n","print (len(y_train),len(y_test))\n","\n","\n","#Transformation\n","X_train = np.array(X_train).reshape(-1,224,224,3)\n","X_test = np.array(X_test).reshape(-1,224,224,3)\n","y_train = to_categorical(y_train)\n","y_test = to_categorical(y_test)\n","\n","\n","#Save data\n","np.save('X_train.npy', X_train)\n","np.save('X_test.npy', X_test)\n","np.save('y_train.npy', y_train)\n","np.save('y_test.npy', y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iWbNoP0JQwdM"},"source":[""],"execution_count":null,"outputs":[]}]}